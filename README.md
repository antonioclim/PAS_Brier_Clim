# `generate_stem_sim.py`: Synthetic Test Generation for Probabilistic Assessment

**Overview:** This Python script generates synthetic multiple-choice tests and associated probabilistic answer data for research in student assessment. It is designed to simulate **STEM** (science, technology, engineering, mathematics) quizzes where each question has several options and an objectively defined correct answer. The output includes a set of **probabilistic truth vectors** (one per question, encoding the correct answer as a probability distribution) and corresponding **belief distributions** for several hypothetical student profiles (archetypes). This allows researchers to explore how different types of test-takers (e.g. confident experts, uninformed guessers, partially knowledgeable students) would respond under a probabilistic scoring scheme. By controlling random seeds and parameters, the script ensures reproducible generation of test data, facilitating rigorous simulation studies and validation of scoring algorithms.

## Running the Script on Windows 11

You can execute `generate_stem_sim.py` on Windows 11 either using PowerShell or within Visual Studio Code’s integrated terminal. In all cases, ensure you have a suitable Python environment set up (Python 3.x installed and added to your PATH).

### Using PowerShell

1. **Open PowerShell:** Click the Start menu, search for “PowerShell”, and launch **Windows PowerShell** (or the newer **Windows Terminal**).  
2. **Navigate to the script directory:** Use the `cd` command to change to the folder containing `generate_stem_sim.py`. For example:  
   ```powershell
   cd C:\Users\YourName\Research\ProbabilisticAssessment\
   ```  
   (Replace the path with the actual directory where the script resides.)  
3. **Run the script with Python:** Invoke the script by typing the Python command followed by the script name and any desired options. For instance:  
   ```powershell
   python generate_stem_sim.py --num_tests 5 --num_questions 20 --seed 42 --output demo_output.csv
   ```  
   This command would generate 5 synthetic tests of 20 questions each, using a random seed of 42, and save the output to `demo_output.csv`. If you omit options, the script will use its built-in default parameters. You can also run `python generate_stem_sim.py -h` to display a help message with all available options and default values.  
4. **Observe output:** Once executed, the script will generate the specified output file in the current directory (or at the given `--output` path). For example, after running the above command, you should find `demo_output.csv` (or a zipped file, as explained below) in the directory. The script may also print a summary of what was generated (e.g., number of tests, questions, etc.) to the console for verification.  
5. **(Optional) Zip the results:** If you included the `--zip` flag in the command, the script will compress the output file. For example, using `--output demo_output.csv --zip` will produce a file `demo_output.zip` containing the CSV. This is useful when the output is large or if you intend to share the results as a single archive.

### Using Visual Studio Code

1. **Open the project in VS Code:** Launch **Visual Studio Code** and open the folder containing `generate_stem_sim.py` (via *File → Open Folder…*). Ensure the Python extension is installed and that VS Code is using the correct Python interpreter for your project.  
2. **Open a terminal in VS Code:** Go to *Terminal → New Terminal* to open an integrated PowerShell terminal (or Command Prompt) at the project directory. The terminal should already be at the correct path if you opened the folder in the previous step.  
3. **Run the script from the terminal:** In the VS Code terminal, execute the script with Python just as you would in a standalone PowerShell. For example:  
   ```powershell
   python generate_stem_sim.py --num_tests 1 --num_questions 10 --output test_simulation.json
   ```  
   This will run the simulation for 1 test of 10 questions and output the data to `test_simulation.json`. You will see any console output (status messages, etc.) directly in the VS Code terminal.  
4. **Alternative – Use the Run button:** As an alternative to using the terminal, you can run the script by opening `generate_stem_sim.py` in the editor and clicking the “Run Python File” button (▶️) provided by the Python extension. This will execute the script with default settings (equivalent to running without command-line arguments). The output file will still be produced as usual in the workspace folder.  
5. **Verify output in VS Code:** After execution, you can refresh the VS Code file explorer to see the generated output file (e.g., CSV or JSON). You may open this file in VS Code to inspect its contents (for instance, to verify the format of the truth vectors and belief distributions). If the `--zip` option was used, ensure you locate the ZIP archive; you can open it with VS Code or a zip utility to see the enclosed data file.

## Configuring Script Parameters

The `generate_stem_sim.py` script exposes several command-line parameters to customize the simulation. By adjusting these, you can control the size and randomness of the generated dataset to fit your experimental needs:

- **Number of tests (`--num_tests`):** Sets how many independent test instances (quiz forms or datasets) to generate. For example, `--num_tests 10` will create ten separate synthetic tests. Each test will have its own set of questions and corresponding data. *(Default: 1 test, unless otherwise specified in the script.)*

- **Number of questions (`--num_questions`):** Sets the number of multiple-choice questions per test. For example, `--num_questions 50` generates a 50-question assessment (each test will contain 50 items). All generated questions will follow the multiple-choice format with a fixed number of options (e.g., 4 options per question, as defined in the script’s internal settings). *(Default: typically 10 or another reasonable number if not provided.)*

- **Random seed (`--seed`):** An integer seed for the random number generator to ensure reproducibility. Providing a seed guarantees that the same “random” test data is generated every run, which is critical for reproducible research. For instance, using `--seed 12345` will always produce the identical set of tests and distributions on each run. If no seed is specified, the script may use a default seed or a system time-based seed (leading to different outputs each run). It is good practice to set this for experiments you plan to share or compare.

- **Output file name (`--output`):** The file path or name for the output data. The script will save the generated test dataset to this file. You can specify a relative or absolute path. For example, `--output sim_data.json` writes the results to a JSON file named *sim_data.json* in the current directory. The format of the output can be inferred from the file extension (e.g., use `.csv` for a comma-separated values file or `.json` for a JSON structured file) – consult the script documentation to see which formats are supported. *(Default: if not given, the script may use a default filename, such as `stem_sim_output.csv`, in the working directory.)*

- **ZIP compression (`--zip` flag):** Include this flag (with no additional value) if you wish to compress the output. When `--zip` is enabled, the script will create a ZIP archive containing the output file instead of leaving the raw output file on disk. For example, `--output sim_data.csv --zip` will result in `sim_data.zip` (containing the CSV file). This feature is useful for bundling results (especially if `--num_tests` or `--num_questions` are large, producing a large data file) and for preparing the simulation output for sharing or publication as supplementary material. *(Default: off; no compression unless explicitly requested.)*

**Example usage:** To illustrate, the following command generates 3 tests each with 30 questions, using a fixed seed of 2025, writing the output to `experiment_dataset.csv` and compressing it:

```powershell
python generate_stem_sim.py --num_tests 3 --num_questions 30 --seed 2025 --output experiment_dataset.csv --zip
This will produce experiment_dataset.zip in the current directory, containing the CSV data. The results will be exactly the same on every run with --seed 2025, allowing other researchers to reproduce the dataset.
Methodology: Test Generation and Data Structure
Synthetic test construction: The script programmatically constructs each test by creating a pool of multiple-choice questions and generating both the “true” answers and simulated student responses for each question. All probabilities are handled in a probabilistic framework, meaning every answer or truth value is represented as a probability distribution across the options (summing to 1). This mirrors the setup in a probabilistic assessment system where students allocate confidence across all options, and there is a well-defined correct answer distribution.
•	Truth vectors: For each question, a truth vector x is generated to represent the correct answer. This vector has length equal to the number of options (e.g., length 4 for a four-option multiple-choice item). The correct option is encoded probabilistically – typically as a 1 for the correct answer and 0 for all other options (a one-hot encoding of the answer key). For example, if option B is the correct answer out of {A, B, C, D}, the truth vector might be x = [0, 1, 0, 0], meaning 0% chance for A, 100% for B, 0% for C, 0% for D. In some cases, the script can also represent partial credit or multi-correct scenarios by distributing weight among options (e.g., x = [0.5, 0.5, 0, 0] could indicate two correct options each 50% correct, or a graded correctness notion). All truth vectors lie on the probability simplex (their components sum to 1), providing a rigorous ground truth for scoring calculations.
•	Student archetypes and belief distributions: The script defines a set of hypothetical student archetypes, each representing a distinct profile of knowledge and confidence. For each archetype and each question, a belief distribution p is generated – this is a probability vector of the same form as the truth vector, but it represents the probabilities that a student of that archetype would assign to each option as their answer. These belief distributions reflect different strategies or levels of understanding:
•	An expert archetype, for instance, might place a very high probability on the correct option and very little on the distractors (e.g., p ≈ [0.02, 0.95, 0.02, 0.01] if the second option is correct, indicating strong confidence in the right answer).
•	A uniform guesser archetype would distribute probabilities roughly evenly across all options (e.g., p ≈ [0.25, 0.25, 0.25, 0.25] for a 4-option question), signifying total uncertainty.
•	A partial-knowledge archetype might assign a moderate probability to one option and spread the rest among others, reflecting uncertainty or common misconceptions (for example, p = [0.1, 0.1, 0.7, 0.1] could represent a student who is fairly sure about option C but not entirely certain).
•	Other archetypes might mimic overconfident but wrong students (high probability on an incorrect option) or underconfident students (who hedge even when they know the answer). These profiles are encoded in the script’s random generation logic by tuning the probability distributions accordingly.
•	Generation process: The script uses the random seed to sample these probabilities in a consistent way. Typically, for each question, it first selects or assigns the correct answer (truth vector). Then for each archetype, it generates a probability vector that has random variation but is biased according to that archetype’s profile relative to the truth. For example, an expert archetype’s distribution might be drawn from a biased Dirichlet distribution concentrated near the truth vector (so that most probability mass falls on the correct answer), whereas a guessing archetype’s distribution might be drawn nearly uniformly. The use of a fixed seed ensures that this stochastic process can be repeated exactly, and if --num_tests > 1, the procedure repeats independently for each test instance.
•	Data organization: The output file collates all this information in a structured format. Each test’s data typically includes:
•	A listing of the questions (often just numbered or indexed, since the content is abstract in simulation).
•	The truth vector x for each question (indicating the correct answer distribution).
•	The set of belief distribution vectors {p_arch} for each archetype (for each question). Depending on the format, these might be presented as separate tables or nested structures. For example, a JSON output might have an array of questions, each containing sub-objects for truth and for each archetype’s probability array. A CSV output might list each question on a row with columns for truth and archetype probabilities, or use multiple CSV files (one for each archetype) packaged together (if so, the --zip flag is especially handy).
In summary, the script produces a rich synthetic dataset that encapsulates both what the correct answers are and how different kinds of students might probabilistically respond to each question. This mirrors the data one would collect in a real probabilistic assessment setting, except here it is fully known and controlled. The methodology corresponds to the “Methods” in a research study, ensuring that any simulations or analyses performed on this data are transparent and replicable.
Research Context and Reproducibility
The primary purpose of generate_stem_sim.py is to support reproducible experiments and simulation-based studies in probabilistic assessment modeling. In an academic research context, using synthetic data is crucial for several reasons:
•	Controlled experiments: Real educational data can be noisy or limited in scope, whereas this script allows generating an unlimited number of test scenarios with precise control over difficulty, randomness, and student behavior profiles. Researchers can craft scenarios to test specific hypotheses (for example, how do extreme guessers fare under logarithmic scoring compared to well-calibrated students?). By adjusting parameters like the number of questions or the archetype distributions, one can simulate tests of varying length and complexity to observe theoretical properties (such as the effect on score variance or reliability as test length increases).
•	Reproducibility: By publishing the script and the random seed alongside a study, other researchers can exactly recreate the dataset used in experiments. This addresses the replicability standards of modern scientific work. For instance, if a paper reports a simulation result based on 1000 simulated students, the authors can share that they ran generate_stem_sim.py with a specific seed and parameter set – any reader can rerun the script with those settings to obtain identical data and verify the findings. This script-based data generation serves as a supplement or reproducibility annex to the research, ensuring that results are not tied to proprietary or irreproducible data.
•	Cross-validation of models and algorithms: The synthetic data can serve as a testbed for validating the implementation of scoring algorithms, estimation methods, or analytical pipelines. Because the “ground truth” is known exactly (we know which option is correct and how the probabilities were assigned), one can verify that a scoring function (for example, the log-loss computation) produces expected values. Indeed, the script can generate known edge-case scenarios – e.g., a uniform guesser vs. an informed student – where the expected outcomes are theoretically calculable. This makes it possible to do consistency checks (for example, confirming that an expert archetype always achieves lower loss than a guesser on the same questions, aligning with the theory of strictly proper scoring rules).
•	Simulation of rare or hypothetical scenarios: The script can create scenarios that might be rare in real life but are useful to study in theory. For example, one could simulate a student who is 100% confident in every answer (to see the impact of overconfidence errors), or a test with an unusually high number of options per question, etc. This flexibility allows stress-testing of the probabilistic assessment framework under various conditions. Because everything is under the experimenter’s control, cause-and-effect can be isolated more easily than in observational data.
In essence, generate_stem_sim.py underpins the research by providing a standardized way to generate test data for probabilistic assessments. It ensures that any simulation results or methodological experiments reported can be replicated exactly, and it integrates with analysis code to facilitate end-to-end reproducible research workflows.
Integration into Analysis Pipelines
The data produced by this simulation script is meant to be readily integrated into analysis pipelines for further computation and interpretation. After generating the synthetic tests, researchers typically feed the output into statistical analysis scripts or interactive notebooks to evaluate performance metrics, test hypotheses, and visualize outcomes. Key aspects of how the generated data can be used include:
•	Loss function evaluation: The probabilistic outputs (truth and belief vectors) are fully compatible with standard loss functions used in probabilistic assessments. For each question, one can compute the logarithmic loss (negative log-likelihood) for a given archetype’s answer by applying the scoring rule to the truth vector x and the archetype’s probability vector p. Summing over questions yields the total quiz loss for that archetype. Because the script simulates various archetypes, you can directly compare these losses to confirm theoretical expectations. For example, an expert archetype’s average loss should be lower than a guesser’s average loss over many questions, reflecting that knowledgeable, well-calibrated students are rewarded under a strictly proper scoring rule. Similarly, one could compute the Brier score (quadratic probability score) for each response vector since the data format (probabilities and binary outcomes in truth vectors) aligns with the inputs to the Brier formula. These evaluations help validate that the scoring metrics behave as intended on the simulated data, and they allow testing of alternative scoring rules or loss functions by plugging in the generated p and x values.
•	Calibration experiments: The synthetic dataset enables detailed calibration analysis. Calibration refers to how well the predicted probabilities align with actual outcomes. In our context, since we know the true answer, we can treat an archetype’s probability assigned to the correct option as a “forecast” of a correct outcome. By grouping questions (or test instances) by the probability level assigned to the correct answer, one can compute how often the question was actually answered correctly in those groups. For instance, if in all cases where a student archetype said “80%” on the correct option, they indeed were correct 80% of the time, then their probabilities are well-calibrated. Using the synthetic data, one can construct calibration curves or reliability diagrams that plot predicted probability vs. observed frequency of correct answers. Because the data is generated from known probability distributions, we expect well-behaved calibration for certain archetypes (e.g., an honest archetype might be well-calibrated by design), whereas others might be intentionally miscalibrated (e.g., an overconfident student might assign 90% to correct answers but only be right 60% of the time). This provides a way to test calibration metrics or correction techniques. The script’s output can be fed into calculations of calibration error, refinement (resolution), and other statistical measures – for example, by using the truth and prediction pairs to compute the calibration component of the Brier score or to run logistic regression for probability calibration.
•	Archetype-specific analysis and interpretation: Since each response in the dataset is labeled by archetype (either explicitly or by how the data is structured per archetype), researchers can perform analysis stratified by archetype. This is extremely valuable for interpreting results:
•	You can calculate summary statistics per archetype, such as the mean score (or loss) for each archetype across all questions or tests. This might show, for example, that the expert archetype achieves an average score corresponding to a high grade, whereas the guesser archetype performs no better than chance. Such results illustrate the efficacy of the scoring rule in differentiating levels of knowledge and confidence.
•	Item-level analysis can also be done: for each question, check how each archetype performed. Did certain tricky questions stump even the high-confidence archetype, or did a particular wrong option attract high probability from the partially knowledgeable archetype? The synthetic data can be used to compute item discrimination indices by treating archetypes as groups – e.g., comparing the probability that the correct answer was identified by strong vs. weak archetypes. Though simulated, this mimics classical item response analysis in a controlled way.
•	The archetype-labelled responses also facilitate qualitative interpretation. For instance, if one archetype consistently misassigns high probability to a particular wrong option, this could be analogous to a misconception in a real educational context. In a research paper, one might discuss how the scoring system penalizes that archetype’s behavior, and what that implies pedagogically. The per-archetype belief patterns generated by the script can be visualized (for example, plotting probability distributions for each archetype on each question) to provide insight into the confidence profiles. This was likely used in the study’s appendix or supplementary materials to demonstrate the variety of student behaviors and how the system responds to each.
•	Pipeline compatibility: Practically, the output of generate_stem_sim.py is structured to be easily read by data analysis libraries (e.g., pandas in Python or data.table in R). If the output is CSV, it can be loaded into a dataframe where each row might correspond to a question (with columns for truth and each archetype’s probabilities). If JSON, it can be parsed into nested objects or converted to tables. The consistent format means that downstream analysis code (for computing losses, plotting graphs, running statistical tests, etc.) can be written once and re-used on any dataset generated by the script. This consistency is crucial for integration into automated pipelines – for example, one could integrate the script into a larger workflow that: (a) generates a dataset, (b) computes performance metrics and calibrations, and (c) repeats this over many random seeds or parameter settings to conduct a Monte Carlo study. Indeed, in the associated research, the authors performed extensive simulations (e.g., thousands of synthetic students with varying parameters) to examine the effects on grade distributions and reliability measures; such experiments would have been infeasible without an automated data generator.
In summary, generate_stem_sim.py is more than just a data generator – it is a foundational tool that connects the theoretical design of a probabilistic student assessment system with empirical analysis. By using this script, researchers ensure that their models and metrics can be tested on a known ground truth dataset, and they can demonstrate, with full transparency, how the probabilistic scoring framework behaves under a variety of simulated conditions. This README has provided guidance on using the script (particularly in a Windows 11 environment) and has outlined how to adjust its parameters and interpret its output. Armed with this tool, specialists can confidently integrate simulation data into their assessment modeling research, whether for verifying loss function implementations, conducting calibration studies, or exploring how different archetypal students interact with innovative scoring mechanisms. The result is a robust, reproducible pipeline from simulation to analysis, strengthening the evidence base for the probabilistic approach to student assessment. ```
